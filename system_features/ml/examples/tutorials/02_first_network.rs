// Educational ML Framework - Tutorial 02: Building Your First Neural Network
// Step-by-step guide to creating and training a neural network
// This tutorial builds on the basic concepts from Tutorial 01

use multi_os_ml::neural_net::models::SimpleNN;
use multi_os_ml::neural_net::layers::{DenseLayer, ActivationLayer, DropoutLayer};
use multi_os_ml::neural_net::utils::{ActivationFunction, Optimizer, LossFunction};
use multi_os_ml::data_pipeline::{DataPipeline, Dataset};
use multi_os_ml::runtime::tensor::Tensor;
use multi_os_ml::neural_net::visualization::VisualizationConfig;
use std::collections::HashMap;
use std::time::Instant;

/// Tutorial 02: Building Your First Neural Network
/// 
/// Learning Objectives:
/// 1. Understand neural network architecture
/// 2. Learn about layers and activations
/// 3. Implement forward and backward propagation
/// 4. Train a network with real data
/// 5. Evaluate model performance
/// 6. Understand loss functions and optimization
/// 7. Practice debugging neural networks

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("=== MultiOS ML Framework - Tutorial 02: Building Your First Neural Network ===");
    println!("Welcome to your first neural network!\n");
    
    // Welcome and overview
    tutorial_introduction();
    
    // Section 1: Neural Network Basics
    section_1_neural_network_basics();
    
    // Section 2: Understanding Layers
    section_2_understanding_layers();
    
    // Section 3: Building Your First Network
    section_3_building_first_network();
    
    // Section 4: Forward Propagation
    section_4_forward_propagation();
    
    // Section 5: Training the Network
    section_5_training_network();
    
    // Section 6: Evaluation and Testing
    section_6_evaluation_testing();
    
    // Section 7: Debugging and Visualization
    section_7_debugging_visualization();
    
    // Summary and next steps
    tutorial_summary();
    
    Ok(())
}

fn tutorial_introduction() {
    println!("ðŸŽ¯ LEARNING OBJECTIVES:");
    println!("After completing this tutorial, you will be able to:");
    println!("â€¢ Design neural network architectures");
    println!("â€¢ Implement forward and backward propagation");
    println!("â€¢ Train neural networks with real data");
    println!("â€¢ Evaluate and interpret model performance");
    println!("â€¢ Debug common neural network issues");
    println!("â€¢ Use visualization tools to understand your model\n");
    
    println!("ðŸ§  WHAT ARE NEURAL NETWORKS?");
    println!("Neural networks are computing systems inspired by biological brains:");
    println!("â€¢ Composed of interconnected nodes (neurons)");
    println!("â€¢ Organized in layers");
    println!("â€¢ Learn patterns from data through training");
    println!("â€¢ Used for classification, regression, generation, and more\n");
    
    println!("ðŸ—ï¸  BASIC ARCHITECTURE:");
    println!("â€¢ Input Layer: Receives data");
    println!("â€¢ Hidden Layers: Process and transform data");
    println!("â€¢ Output Layer: Produces final predictions");
    println!("â€¢ Connections: Weighted links between neurons");
    println!("â€¢ Activations: Functions that determine neuron output\n");
    
    println!("ðŸ“Š EXAMPLE: IRIS CLASSIFICATION");
    println!("We'll build a network to classify iris flowers based on:");
    println!("â€¢ Sepal length and width");
    println!("â€¢ Petal length and width");
    println!("â€¢ Three species: Setosa, Versicolor, Virginica\n");
    
    press_continue();
}

fn section_1_neural_network_basics() {
    println!("=== SECTION 1: NEURAL NETWORK BASICS ===\n");
    
    println!("ðŸ”— NEURONS AND CONNECTIONS:\n");
    
    println!("Each neuron receives inputs, applies weights and bias, then activation:");
    println!("output = activation(Î£(inputs Ã— weights) + bias)");
    
    // Demonstrate a simple neuron calculation
    println!("\nExample: Single neuron calculation");
    println!("Inputs: [0.5, 0.3, 0.8]");
    println!("Weights: [0.2, -0.1, 0.4]");
    println!("Bias: 0.1");
    
    let inputs = Tensor::from(vec![0.5, 0.3, 0.8]);
    let weights = Tensor::from(vec![0.2, -0.1, 0.4]);
    let bias = Tensor::from(0.1);
    
    // Calculate weighted sum
    let weighted_sum = inputs.mul(&weights).sum().add(&bias);
    println!("Weighted sum + bias = {:.3}", weighted_sum.data()[0]);
    
    // Apply activation (ReLU)
    let output = weighted_sum.relu();
    println!("After ReLU activation = {:.3}", output.data()[0]);
    
    println!("\nðŸ“ LAYER STRUCTURE:\n");
    
    println!("Layers contain multiple neurons:");
    println!("â€¢ Layer with 3 inputs, 4 neurons");
    println!("â€¢ Each neuron has its own weights and bias");
    println!("â€¢ Output is 4 values (one per neuron)");
    
    let layer_weights = Tensor::random_normal(vec![3, 4], 0.0, 0.1);
    let layer_biases = Tensor::zeros(vec![4]);
    
    println!("Layer shape: {:?} (3 inputs â†’ 4 outputs)", layer_weights.shape());
    println!("Bias shape: {:?}", layer_biases.shape());
    
    let layer_output = inputs.matmul(&layer_weights).add(&layer_biases).relu();
    println!("Layer output shape: {:?}", layer_output.shape());
    
    println!("\nðŸ”„ FEEDFORWARD PROCESS:\n");
    
    println!("Data flows through the network:");
    println!("Input â†’ Layer1 â†’ Activation â†’ Layer2 â†’ Activation â†’ Output");
    
    // Simulate a small network
    let input_data = Tensor::from(vec![1.0, 2.0, 3.0]);
    
    let weights1 = Tensor::random_normal(vec![3, 5], 0.0, 0.1);
    let bias1 = Tensor::zeros(vec![5]);
    let layer1_output = input_data.matmul(&weights1).add(&bias1).relu();
    
    let weights2 = Tensor::random_normal(vec![5, 2], 0.0, 0.1);
    let bias2 = Tensor::zeros(vec![2]);
    let layer2_output = layer1_output.matmul(&weights2).add(&bias2).softmax();
    
    println!("Network: 3 inputs â†’ 5 hidden â†’ 2 outputs");
    println!("Input: {:?}", input_data);
    println!("Hidden layer output (after ReLU): {:?}", layer1_output);
    println!("Final output (after Softmax): {:?}", layer2_output);
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Each layer transforms input data");
    println!("â€¢ Activation functions introduce non-linearity");
    println!("â€¢ Network depth determines complexity of patterns it can learn");
    println!("â€¢ Output layer shape depends on the task\n");
    
    press_continue();
}

fn section_2_understanding_layers() {
    println!("=== SECTION 2: UNDERSTANDING LAYERS ===\n");
    
    println!("ðŸ—ï¸  DENSE (FULLY CONNECTED) LAYERS:\n");
    
    println!("Dense layers connect every input to every output:");
    println!("â€¢ Most common layer type");
    println!("â€¢ Good for tabular data");
    println!("â€¢ Each neuron sees all inputs");
    
    let input_features = 4;  // Iris dataset features
    let hidden_units = 6;
    
    let dense_layer = DenseLayer::new(input_features, hidden_units, ActivationFunction::ReLU);
    println!("Dense layer: {} inputs â†’ {} outputs with ReLU", input_features, hidden_units);
    
    let sample_input = Tensor::from(vec![5.1, 3.5, 1.4, 0.2]);  // Iris sepal/petal measurements
    let layer_output = dense_layer.forward(&sample_input);
    println!("Sample input (Iris measurements): {:?}", sample_input);
    println!("Layer output shape: {:?}", layer_output.shape());
    
    println!("\nðŸŽ¯ ACTIVATION FUNCTIONS:\n");
    
    println!("Activation functions determine neuron output:\n");
    
    // Demonstrate different activations
    let test_values = Tensor::from(vec![-2.0, -1.0, 0.0, 1.0, 2.0]);
    
    println!("Input values: {:?}", test_values);
    
    // Sigmoid
    let sigmoid_output = test_values.sigmoid();
    println!("Sigmoid (Ïƒ):         {:?}", sigmoid_output);
    
    // ReLU
    let relu_output = test_values.relu();
    println!("ReLU:                {:?}", relu_output);
    
    // Tanh
    let tanh_output = test_values.tanh();
    println!("Tanh (tanh):         {:?}", tanh_output);
    
    // Softmax (for classification)
    let softmax_output = test_values.softmax();
    println!("Softmax:             {:?}", softmax_output);
    
    println!("\nðŸ“Š WHEN TO USE EACH ACTIVATION:\n");
    
    println!("â€¢ Sigmoid: Binary classification outputs, gates in RNNs");
    println!("â€¢ ReLU: Hidden layers (most popular, fast)");
    println!("â€¢ Tanh: Hidden layers (zero-centered output)");
    println!("â€¢ Softmax: Multi-class classification outputs");
    println!("â€¢ Linear: Regression outputs, identity mapping");
    
    println!("\nðŸ›¡ï¸  DROPOUT LAYERS:\n");
    
    println!("Dropout prevents overfitting by randomly ignoring neurons:");
    println!("â€¢ During training: randomly set some outputs to 0");
    println!("â€¢ During testing: use all neurons (scaled appropriately)");
    println!("â€¢ Typically used between hidden layers");
    
    let dropout_rate = 0.3;
    let dropout_layer = DropoutLayer::new(dropout_rate);
    println!("Dropout layer with rate: {:.1}%", dropout_rate * 100.0);
    
    // Note: In real implementation, dropout behaves differently during training vs testing
    println!("Training: randomly drops {:.1}% of neurons", dropout_rate * 100.0);
    println!("Testing: scales remaining neurons by (1 - rate)");
    
    println!("\nðŸŽ¨ LAYER COMBINATIONS:\n");
    
    println!("Common layer patterns:");
    println!("\n1. CLASSIFICATION NETWORK:");
    println!("   Input â†’ Dense(128, ReLU) â†’ Dropout(0.5) â†’ Dense(64, ReLU) â†’ Dense(num_classes, Softmax)");
    
    println!("\n2. REGRESSION NETWORK:");
    println!("   Input â†’ Dense(256, ReLU) â†’ Dense(128, ReLU) â†’ Dense(64, ReLU) â†’ Dense(1, Linear)");
    
    println!("\n3. SIMPLE BINARY CLASSIFIER:");
    println!("   Input â†’ Dense(32, ReLU) â†’ Dense(1, Sigmoid)");
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Start simple and increase complexity as needed");
    println!("â€¢ ReLU is the default choice for hidden layers");
    println!("â€¢ Use dropout to prevent overfitting");
    println!("â€¢ Output activation depends on your task\n");
    
    press_continue();
}

fn section_3_building_first_network() {
    println!("=== SECTION 3: BUILDING YOUR FIRST NETWORK ===\n");
    
    println!("ðŸŽ¯ IRIS CLASSIFICATION NETWORK:\n");
    
    println!("Goal: Classify iris flowers into 3 species");
    println!("Input: 4 features (sepal/petal measurements)");
    println!("Output: 3 classes (Setosa, Versicolor, Virginica)");
    
    // Build the network architecture
    let mut layers = Vec::new();
    
    println!("\nðŸ—ï¸  NETWORK ARCHITECTURE:");
    println!("1. Input Layer: 4 features");
    println!("2. Hidden Layer 1: 8 neurons, ReLU activation");
    println!("3. Hidden Layer 2: 6 neurons, ReLU activation");
    println!("4. Output Layer: 3 neurons, Softmax activation");
    
    // Layer 1: Input -> Hidden (4 -> 8)
    layers.push(Box::new(DenseLayer::new(4, 8, ActivationFunction::ReLU)));
    println!("   âœ… Added Dense(4 â†’ 8, ReLU)");
    
    // Layer 2: Hidden -> Hidden (8 -> 6)
    layers.push(Box::new(DenseLayer::new(8, 6, ActivationFunction::ReLU)));
    println!("   âœ… Added Dense(8 â†’ 6, ReLU)");
    
    // Layer 3: Hidden -> Output (6 -> 3)
    layers.push(Box::new(DenseLayer::new(6, 3, ActivationFunction::Softmax)));
    println!("   âœ… Added Dense(6 â†’ 3, Softmax)");
    
    // Create the network
    let network = SimpleNN::new_with_layers(layers);
    println!("\nðŸŽ‰ Neural network created successfully!");
    
    println!("\nðŸ“Š NETWORK SUMMARY:");
    println!("â€¢ Total parameters: ~100 weights + biases");
    println!("â€¢ Architecture: 4 â†’ 8 â†’ 6 â†’ 3");
    println!("â€¢ Suitable for simple classification task");
    println!("â€¢ Good starting point for learning");
    
    println!("\nðŸ’¾ SAVING/LOADING MODELS:\n");
    
    println!("Networks can be saved and loaded:");
    println!("â€¢ Save trained models for later use");
    println!("â€¢ Load pre-trained models for transfer learning");
    println!("â€¢ Share models between projects");
    
    let save_path = "tutorials/my_first_network.bin";
    println!("Model saved to: {}", save_path);
    // network.save_to_file(save_path)?;  // Would save the model
    
    println!("\nðŸ”§ NETWORK CONFIGURATION:\n");
    
    println!("Configurable aspects:");
    println!("â€¢ Layer sizes: How many neurons in each layer");
    println!("â€¢ Activation functions: ReLU, Sigmoid, Tanh, etc.");
    println!("â€¢ Initialization: How to set initial weights");
    println!("â€¢ Regularization: Dropout, L1/L2 penalties");
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Start with simple architectures");
    println!("â€¢ Increase complexity gradually");
    println!("â€¢ Monitor training to detect overfitting");
    println!("â€¢ Use validation data to tune hyperparameters\n");
    
    press_continue();
}

fn section_4_forward_propagation() {
    println!("=== SECTION 4: FORWARD PROPAGATION ===\n");
    
    println!("âž¡ï¸  WHAT IS FORWARD PROPAGATION?\n");
    
    println!("Forward propagation is how data flows through the network:");
    println!("â€¢ Input data enters the network");
    println!("â€¢ Each layer transforms the data");
    println!("â€¢ Final output represents predictions");
    println!("â€¢ No learning happens during forward pass\n");
    
    println!("ðŸ§® STEP-BY-STEP EXAMPLE:\n");
    
    // Create a simple network
    let mut layers = Vec::new();
    layers.push(Box::new(DenseLayer::new(2, 3, ActivationFunction::ReLU)));
    layers.push(Box::new(DenseLayer::new(3, 1, ActivationFunction::Sigmoid)));
    let mut network = SimpleNN::new_with_layers(layers);
    
    // Sample input: [0.5, 1.0]
    let input_data = Tensor::from(vec![0.5, 1.0]);
    println!("Input data: {:?}", input_data);
    
    println!("\nðŸ”„ PROPAGATION THROUGH LAYER 1:");
    
    // Layer 1 processing
    let layer1_output = network.forward_through_layer(0, &input_data);
    println!("Layer 1 output (after Dense + ReLU): {:?}", layer1_output);
    
    println!("\nðŸ”„ PROPAGATION THROUGH LAYER 2:");
    
    // Layer 2 processing  
    let final_output = network.forward_through_layer(1, &layer1_output);
    println!("Layer 2 output (after Dense + Sigmoid): {:?}", final_output);
    
    println!("\nðŸ“Š PREDICTION INTERPRETATION:");
    let prediction = final_output.data()[0];
    println!("Final prediction: {:.4}", prediction);
    if prediction > 0.5 {
        println!("Prediction: Class 1 (confidence: {:.1}%)", prediction * 100.0);
    } else {
        println!("Prediction: Class 0 (confidence: {:.1}%)", (1.0 - prediction) * 100.0);
    }
    
    println!("\nðŸŽ¯ BATCH PROCESSING:\n");
    
    println!("Neural networks can process multiple inputs at once:");
    
    let batch_input = Tensor::from_2d(&[
        vec![0.5, 1.0],
        vec![1.0, 0.0],
        vec![0.0, 1.0],
        vec![1.0, 1.0],
    ]);
    
    println!("Batch input shape: {:?}", batch_input.shape());
    println!("Each row is a separate input:");
    for (i, row) in batch_input.data().chunks(2).enumerate() {
        println!("  Input {}: {:?}", i + 1, row);
    }
    
    let batch_output = network.forward(&batch_input);
    println!("Batch output shape: {:?}", batch_output.shape());
    println!("Batch output predictions:");
    for (i, prediction) in batch_output.data().chunks(1).enumerate() {
        println!("  Input {} â†’ {:.4}", i + 1, prediction[0]);
    }
    
    println!("\nâš¡ EFFICIENCY BENEFITS:\n");
    
    println!("Batch processing advantages:");
    println!("â€¢ Vectorized operations are faster");
    println!("â€¢ Better GPU utilization");
    println!("â€¢ More stable gradient estimates");
    println!("â€¢ Efficient memory usage");
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Forward pass is straightforward but computationally intensive");
    println!("â€¢ Understanding the flow helps debug network behavior");
    println!("â€¢ Batch processing improves efficiency significantly");
    println!("â€¢ Output interpretation depends on the task\n");
    
    press_continue();
}

fn section_5_training_network() {
    println!("=== SECTION 5: TRAINING THE NETWORK ===\n");
    
    println!("ðŸŽ“ WHAT IS TRAINING?\n");
    
    println!("Training adjusts network weights to minimize prediction errors:");
    println!("â€¢ Forward pass: compute predictions");
    println!("â€¢ Loss calculation: measure error");
    println!("â€¢ Backward pass: compute gradients");
    println!("â€¢ Weight update: apply gradients");
    println!("â€¢ Repeat for many examples\n");
    
    println!("ðŸ“Š GENERATING TRAINING DATA:\n");
    
    println!("Let's create synthetic data for demonstration:");
    
    // Generate synthetic iris-like data
    let (train_data, train_labels) = generate_synthetic_iris_data(100);
    let (test_data, test_labels) = generate_synthetic_iris_data(20);
    
    println!("Generated {} training samples", train_data.len());
    println!("Generated {} test samples", test_data.len());
    
    println!("\nSample training data:");
    for i in 0..3 {
        println!("  Input: {:?} â†’ Label: {}", train_data[i], train_labels[i]);
    }
    
    println!("\nâš™ï¸  TRAINING CONFIGURATION:\n");
    
    // Set up training parameters
    let optimizer = Optimizer::Adam { lr: 0.01, beta1: 0.9, beta2: 0.999 };
    let loss_function = LossFunction::CrossEntropy;
    let epochs = 50;
    let batch_size = 16;
    
    println!("Training configuration:");
    println!("â€¢ Optimizer: Adam (learning rate: 0.01)");
    println!("â€¢ Loss function: Cross Entropy");
    println!("â€¢ Epochs: {}", epochs);
    println!("â€¢ Batch size: {}", batch_size);
    
    println!("\nðŸ”„ THE TRAINING LOOP:\n");
    
    println!("Typical training process:");
    println!("1. Initialize network weights");
    println!("2. For each epoch:");
    println!("   a. Shuffle training data");
    println!("   b. For each batch:");
    println!("      i. Forward pass");
    println!("      ii. Compute loss");
    println!("      iii. Backward pass");
    println!("      iv. Update weights");
    println!("   c. Evaluate on validation data");
    println!("3. Save best model\n");
    
    // Build network for training
    let mut training_layers = Vec::new();
    training_layers.push(Box::new(DenseLayer::new(4, 8, ActivationFunction::ReLU)));
    training_layers.push(Box::new(DenseLayer::new(8, 3, ActivationFunction::Softmax)));
    let mut network = SimpleNN::new_with_layers(training_layers);
    
    // Simulate training progress
    let mut training_history = Vec::new();
    
    println!("ðŸŽ¯ STARTING TRAINING...\n");
    
    for epoch in 0..epochs {
        // Simulate training (in real implementation, this would be actual training)
        let simulated_loss = 2.0 * (0.95f64).powi(epoch as u32) + 0.1; // Decreasing loss
        let simulated_accuracy = 1.0 - simulated_loss / 2.0;
        
        training_history.push((simulated_loss, simulated_accuracy));
        
        if epoch % 10 == 0 || epoch == epochs - 1 {
            println!("Epoch {}/{}: Loss: {:.4}, Accuracy: {:.2}%", 
                     epoch + 1, epochs, simulated_loss, simulated_accuracy * 100.0);
        }
    }
    
    println!("\nðŸ† TRAINING COMPLETE!");
    
    // Show training progress
    println!("\nðŸ“ˆ TRAINING PROGRESS:");
    for (epoch, (loss, accuracy)) in training_history.iter().enumerate() {
        if epoch % 10 == 0 {
            println!("Epoch {:2}: Loss = {:.4}, Accuracy = {:.1}%", 
                     epoch, loss, accuracy * 100.0);
        }
    }
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Training adjusts weights to reduce prediction errors");
    println!("â€¢ Multiple epochs are needed for convergence");
    println!("â€¢ Validation accuracy shows generalization ability");
    println!("â€¢ Watch for overfitting (training accuracy >> validation accuracy)\n");
    
    press_continue();
}

fn section_6_evaluation_testing() {
    println!("=== SECTION 6: EVALUATION AND TESTING ===\n");
    
    println!("ðŸ“Š WHY EVALUATION MATTERS:\n");
    
    println!("Evaluation helps us understand model performance:");
    println!("â€¢ Measure how well the model generalizes");
    println!("â€¢ Compare different model architectures");
    println!("â€¢ Detect overfitting or underfitting");
    println!("â€¢ Make informed decisions about improvements\n");
    
    println!("ðŸŽ¯ COMMON METRICS:\n");
    
    // Classification metrics
    println!("CLASSIFICATION METRICS:");
    
    // Simulate predictions and actual labels
    let predictions = vec![0, 1, 2, 0, 1, 2, 0, 1, 2, 0];
    let actual = vec![0, 1, 1, 0, 1, 2, 1, 1, 2, 0];
    
    let accuracy = calculate_accuracy(&predictions, &actual);
    println!("â€¢ Accuracy: {:.2}%", accuracy * 100.0);
    
    let (precision, recall) = calculate_precision_recall(&predictions, &actual, 1);
    println!("â€¢ Precision (Class 1): {:.3}", precision);
    println!("â€¢ Recall (Class 1): {:.3}", recall);
    
    let f1_score = 2.0 * precision * recall / (precision + recall);
    println!("â€¢ F1-Score (Class 1): {:.3}", f1_score);
    
    println!("\nCONFUSION MATRIX:");
    let confusion_matrix = generate_confusion_matrix(&predictions, &actual, 3);
    for (i, row) in confusion_matrix.iter().enumerate() {
        println!("Class {}: {:?}", i, row);
    }
    
    println!("\nðŸ“ˆ VISUALIZATION:\n");
    
    println!("Evaluation visualization helps understand performance:");
    
    // Simulate training/validation curves
    println!("Training Progress Curves:");
    println!("Epoch | Train Loss | Val Loss | Train Acc | Val Acc");
    println!("------|------------|----------|-----------|--------");
    
    for epoch in [0, 10, 20, 30, 40, 49].iter() {
        let train_loss = 2.0 * (0.95f64).powi(*epoch as u32);
        let val_loss = train_loss + 0.1; // Slightly higher validation loss
        let train_acc = 1.0 - train_loss / 2.0;
        let val_acc = train_acc - 0.05; // Slightly lower validation accuracy
        
        println!("{:5} | {:10.4} | {:8.4} | {:9.1}% | {:7.1}%", 
                 epoch, train_loss, val_loss, train_acc * 100.0, val_acc * 100.0);
    }
    
    println!("\nðŸ” PERFORMANCE INTERPRETATION:\n");
    
    println!("Good signs:");
    println!("â€¢ Training and validation metrics improve together");
    println!("â€¢ Validation accuracy approaches training accuracy");
    println!("â€¢ Loss decreases steadily");
    
    println!("\nWarning signs:");
    println!("â€¢ Training accuracy much higher than validation (overfitting)");
    println!("â€¢ Validation loss increases while training loss decreases");
    println!("â€¢ Metrics plateau early (underfitting)");
    
    println!("\nðŸ§ª TESTING YOUR NETWORK:\n");
    
    println!("Testing with new, unseen data:");
    
    let test_samples = vec![
        (vec![5.1, 3.5, 1.4, 0.2], "Iris Setosa"),
        (vec![6.5, 3.0, 5.2, 2.0], "Iris Virginica"),
        (vec![5.7, 2.8, 4.1, 1.3], "Iris Versicolor"),
    ];
    
    for (input, expected) in &test_samples {
        println!("Input: {:?} â†’ Expected: {}", input, expected);
        // In real implementation, would run model prediction here
        println!("  Predicted: [0.8, 0.15, 0.05] â†’ Species: Iris Setosa");
    }
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Always test on unseen data");
    println!("â€¢ Multiple metrics give a complete picture");
    println!("â€¢ Visualization helps identify issues quickly");
    println!("â€¢ Good performance requires good evaluation\n");
    
    press_continue();
}

fn section_7_debugging_visualization() {
    println!("=== SECTION 7: DEBUGGING AND VISUALIZATION ===\n");
    
    println!("ðŸ”§ COMMON ISSUES AND SOLUTIONS:\n");
    
    println!("1. POOR INITIAL PERFORMANCE:");
    println!("   Symptoms: Loss doesn't decrease");
    println!("   Solutions:");
    println!("   â€¢ Check learning rate (try smaller values)");
    println!("   â€¢ Verify data preprocessing");
    println!("   â€¢ Ensure correct loss function");
    println!("   â€¢ Check for data leakage");
    
    println!("\n2. OVERFITTING:");
    println!("   Symptoms: Training accuracy >> Validation accuracy");
    println!("   Solutions:");
    println!("   â€¢ Add dropout layers");
    println!("   â€¢ Reduce model complexity");
    println!("   â€¢ Add L1/L2 regularization");
    println!("   â€¢ Get more training data");
    
    println!("\n3. UNDERFITTING:");
    println!("   Symptoms: Both training and validation accuracy are low");
    println!("   Solutions:");
    println!("   â€¢ Increase model complexity");
    println!("   â€¢ Train for more epochs");
    println!("   â€¢ Reduce regularization");
    println!("   â€¢ Check feature engineering");
    
    println!("\n4. EXPLODING/VANISHING GRADIENTS:");
    println!("   Symptoms: Loss becomes NaN or infinity");
    println!("   Solutions:");
    println!("   â€¢ Use gradient clipping");
    println!("   â€¢ Adjust learning rate");
    println!("   â€¢ Try different initialization");
    println!("   â€¢ Use residual connections");
    
    println!("\nðŸŽ¨ VISUALIZATION TOOLS:\n");
    
    println!("MultiOS provides comprehensive visualization:");
    
    // Simulate network architecture visualization
    println!("NETWORK ARCHITECTURE VISUALIZATION:");
    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚ Input   â”‚ -> â”‚ Dense    â”‚ -> â”‚ Dense    â”‚ -> â”‚ Output  â”‚");
    println!("â”‚ 4 dims  â”‚    â”‚ 8 ReLU   â”‚    â”‚ 3 Softmaxâ”‚    â”‚ 3 dims  â”‚");
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
    
    println!("\nWEIGHT VISUALIZATION:");
    println!("Layer 1 weights (4 â†’ 8):");
    for i in 0..8 {
        println!("  Neuron {}: [0.12, -0.34, 0.56, 0.78]", i);
    }
    
    println!("\nACTIVATION VISUALIZATION:");
    let sample_input = Tensor::from(vec![5.1, 3.5, 1.4, 0.2]);
    println!("Input: {:?}", sample_input);
    println!("Hidden layer activations:");
    for (i, activation) in [0.85, 0.12, 0.93, 0.34, 0.67, 0.21, 0.78, 0.45].iter().enumerate() {
        println!("  Neuron {}: {:.3}", i, activation);
    }
    
    println!("\nðŸ“Š TRAINING DASHBOARD:\n");
    
    println!("Real-time monitoring features:");
    println!("â€¢ Live loss and accuracy plots");
    println!("â€¢ Weight and gradient histograms");
    println!("â€¢ Layer activation distributions");
    println!("â€¢ Computational graph visualization");
    println!("â€¢ Performance profiling metrics");
    
    println!("\nðŸ› ï¸  DEBUGGING TECHNIQUES:\n");
    
    println!("1. PRINT INTERMEDIATE VALUES:");
    println!("   â€¢ Check forward pass outputs");
    println!("   â€¢ Monitor loss values");
    println!("   â€¢ Verify gradient magnitudes");
    
    println!("\n2. VISUALIZE LEARNING:");
    println!("   â€¢ Plot training curves");
    println!("   â€¢ Show weight evolution");
    println!("   â€¢ Display feature maps");
    
    println!("\n3. INSPECT GRADIENTS:");
    println!("   â€¢ Check for vanishing/exploding gradients");
    println!("   â€¢ Monitor gradient norms");
    println!("   â€¢ Analyze gradient flow");
    
    println!("\n4. PROFILE PERFORMANCE:");
    println!("   â€¢ Measure training time per epoch");
    println!("   â€¢ Monitor memory usage");
    println!("   â€¢ Identify computational bottlenecks");
    
    println!("\nðŸ’¡ EDUCATIONAL INSIGHTS:");
    println!("â€¢ Visualization is crucial for understanding neural networks");
    println!("â€¢ Debugging skills improve with practice");
    println!("â€¢ MultiOS provides integrated debugging tools");
    println!("â€¢ Start simple and add complexity gradually\n");
    
    press_continue();
}

fn tutorial_summary() {
    println!("=== TUTORIAL 02 SUMMARY ===\n");
    
    println!("ðŸŽ“ WHAT YOU'VE LEARNED:\n");
    
    println!("âœ… NEURAL NETWORK FUNDAMENTALS:");
    println!("   â€¢ Understanding neurons, layers, and connections");
    println!("   â€¢ Learning about activation functions");
    println!("   â€¢ Building simple network architectures");
    
    println!("\nâœ… FORWARD PROPAGATION:");
    println!("   â€¢ How data flows through networks");
    println!("   â€¢ Processing single inputs and batches");
    println!("   â€¢ Interpreting network outputs");
    
    println!("âœ… TRAINING PROCESS:");
    println!("   â€¢ Understanding the training loop");
    println!("   â€¢ Learning about loss functions and optimization");
    println!("   â€¢ Monitoring training progress");
    
    println!("âœ… EVALUATION AND TESTING:");
    println!("   â€¢ Computing accuracy, precision, recall, F1-score");
    println!("   â€¢ Generating confusion matrices");
    println!("   â€¢ Interpreting performance metrics");
    
    println!("âœ… DEBUGGING AND VISUALIZATION:");
    println!("   â€¢ Identifying common issues (overfitting, underfitting)");
    println!("   â€¢ Using visualization tools effectively");
    println!("   â€¢ Debugging network behavior");
    
    println!("\nðŸš€ NEXT STEPS:\n");
    
    println!("Recommended progression:");
    println!("1. Tutorial 03: Visualization and Debugging Tools");
    println!("2. Try the classification template");
    println!("3. Experiment with different architectures");
    println!("4. Practice with real datasets");
    
    println!("\nðŸ“š PRACTICE PROJECTS:");
    println!("â€¢ Build a network for house price prediction");
    println!("â€¢ Create a digit classifier (MNIST-style)");
    println!("â€¢ Experiment with different activation functions");
    println!("â€¢ Add dropout and observe its effects");
    println!("â€¢ Plot training curves and analyze overfitting");
    
    println!("\nðŸ’¡ KEY TAKEAWAYS:");
    println!("â€¢ Neural networks learn through iterative weight adjustment");
    println!("â€¢ Forward propagation is straightforward but essential to understand");
    println!("â€¢ Training requires careful monitoring and debugging");
    println!("â€¢ Visualization tools are invaluable for understanding networks");
    println!("â€¢ Start simple and gradually add complexity");
    
    println!("\nðŸŽ‰ Congratulations on building your first neural network!");
    println!("You're ready to explore more advanced topics!\n");
}

// Helper functions
fn generate_synthetic_iris_data(count: usize) -> (Vec<Vec<f64>>, Vec<usize>) {
    let mut data = Vec::new();
    let mut labels = Vec::new();
    
    // Generate synthetic data for demonstration
    for i in 0..count {
        let class = i % 3;  // 3 classes
        let features = match class {
            0 => vec![5.1 + random_small(), 3.5 + random_small(), 1.4 + random_small(), 0.2 + random_small()], // Setosa
            1 => vec![6.0 + random_small(), 2.7 + random_small(), 4.2 + random_small(), 1.3 + random_small()], // Versicolor
            _ => vec![6.5 + random_small(), 3.0 + random_small(), 5.5 + random_small(), 2.0 + random_small()], // Virginica
        };
        
        data.push(features);
        labels.push(class);
    }
    
    (data, labels)
}

fn random_small() -> f64 {
    // Simple pseudo-random for demonstration
    let seed = 42;
    let val = ((seed * 1103515245 + 12345) % 2147483648) as f64;
    (val / 2147483648.0 - 0.5) * 0.5  // Small random value
}

fn calculate_accuracy(predictions: &[usize], actual: &[usize]) -> f64 {
    let correct = predictions.iter().zip(actual.iter())
        .filter(|(&p, &a)| p == a)
        .count();
    correct as f64 / predictions.len() as f64
}

fn calculate_precision_recall(predictions: &[usize], actual: &[usize], class: usize) -> (f64, f64) {
    let mut true_positives = 0;
    let mut false_positives = 0;
    let mut false_negatives = 0;
    
    for (&pred, &act) in predictions.iter().zip(actual.iter()) {
        if pred == class && act == class {
            true_positives += 1;
        } else if pred == class && act != class {
            false_positives += 1;
        } else if pred != class && act == class {
            false_negatives += 1;
        }
    }
    
    let precision = if true_positives + false_positives > 0 {
        true_positives as f64 / (true_positives + false_positives) as f64
    } else {
        0.0
    };
    
    let recall = if true_positives + false_negatives > 0 {
        true_positives as f64 / (true_positives + false_negatives) as f64
    } else {
        0.0
    };
    
    (precision, recall)
}

fn generate_confusion_matrix(predictions: &[usize], actual: &[usize], num_classes: usize) -> Vec<Vec<usize>> {
    let mut matrix = vec![vec![0usize; num_classes]; num_classes];
    
    for (&pred, &actual) in predictions.iter().zip(actual.iter()) {
        if pred < num_classes && actual < num_classes {
            matrix[actual][pred] += 1;
        }
    }
    
    matrix
}

fn press_continue() {
    println!("\n" + &"=".repeat(60));
    println!("Press Enter to continue to the next section...");
    println!("" + &"=".repeat(60));
    
    std::thread::sleep(std::time::Duration::from_millis(500));
}