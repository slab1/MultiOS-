{"origin_pdf_path": "https://www.usenix.org/system/files/osdi24-chen-haibo.pdf", "text_in_pdf": "Microkernel Goes General: Performance and Compatibility in the HongMeng Production Microkernel  \n\nHaibo Chen, Huawei Central Software Institute and Shanghai Jiao Tong University;   \nXie Miao, Ning Jia, Nan Wang, Yu Li, Nian Liu, Yutao Liu, Fei Wang, Qiang Huang, Kun Li, Hongyang Yang, Hui Wang, Jie Yin, Yu Peng, and Fengwei Xu, Huawei Central Software Institute  \n\nhttps://www.usenix.org/conference/osdi24/presentation/chen-haibo  \n\nThis paper is included in the Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation.  \n\nJuly 10–12, 2024 • Santa Clara, CA, USA 978-1-939133-40-3  \n\nOpen access to the Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation is sponsored by  \n\nMicrokernel Goes General: Performance and Compatibility in the HongMeng Production Microkernel  \n\nHaibo Chen1,2, Xie Miao1, Ning Jia1, Nan Wang1, $\\mathrm{Yu}\\,\\mathrm{Li}^{1}$ , Nian $\\mathrm{Liu}^{1}$ , Yutao $\\mathrm{Liu}^{1}$ , Fei Wang1, Qiang Huang1, Kun $\\mathrm{Li}^{1}$ , Hongyang Yang1, Hui $\\mathrm{Wang}^{1}$ , Jie $\\mathrm{Yin}^{1}$ , $\\mathrm{Yu\\,Peng^{1}}$ , and Fengwei $\\mathrm{Xu}^{1}$  \n\n1Huawei Central Software Institute, 2Shanghai Jiao Tong University  \n\nAbstract  \n\nThe virtues of security, reliability, and extensibility have made state-of-the-art microkernels prevalent in embedded and safety-critical scenarios. However, they face performance and compatibility issues when targeting more general scenarios, such as smartphones and smart vehicles.  \n\nThis paper presents the design and implementation of HongMeng kernel (HM), a commercialized general-purpose microkernel that preserves most of the virtues of microkernels while addressing the above challenges. For the sake of commercial practicality, we design HM to be compatible with the Linux API and ABI to reuse its rich applications and driver ecosystems. To make it performant despite the constraints of compatibility and being general-purpose, we re-examine the traditional microkernel wisdom, including IPC, capabilitybased access control, and userspace paging, and retrofti them accordingly. Specifically, we argue that per-invocation IPC is not the only concern for performance, but IPC frequency, state double bookkeeping among OS services, and capabilities that hide kernel objects contribute to significant performance degradation. We mitigate them accordingly with a set of techniques, including differentiated isolation classes, flexible composition, policy-free kernel paging, and address-token-based access control.  \n\nHM consists of a minimal core kernel and a set of leastprivileged OS services, and it can run complex frameworks like AOSP and OpenHarmony. HM has been deployed in production on tens of millions of devices in emerging scenarios, including smart routers, smart vehicles and smartphones, typically with improved performance and security over their Linux counterparts.  \n\n1 Introduction  \n\nMicrokernels minimize functionality in the kernel and move components, such as flie systems and device drivers, into wellisolated and least-privileged OS services, achieving better reliability, security, and extensibility than monolithic kernels such as Linux. Thanks to these virtues, state-of-the-art (SOTA) microkernels have been widely deployed in embedded and safety-critical scenarios [30,52,54].  \n\nOn the other hand, while monolithic kernels like Linux dominate in general-purpose scenarios such as servers and the cloud, there are increasingly emerging scenarios such as smart vehicles and smartphones that require better security, reliability, and extensibility in addition to good performance, where Linux is less suitable. While being general, Linux evolves more towards servers and the cloud, making other scenarios less beneficial. For example, it took over 10 years for the preemptive-RT patch [1] to be partially merged, and its evolution is still out of the mainstream, let alone other domainspecific strategies [20,21]. Moreover, it has been doomed to be difficult (if possible) for Linux to satisfy high-level industry certifications required for such scenarios [98,113].  \n\nHowever, although microkernels have been extensively studied for decades [16, 28, 30, 46, 49, 52, 52–54, 64, 67, 73, 75,76,86], SOTA microkernels mainly target some specific domains, e.g., embedded and safety-critical ones. They usually use static resource partitioning and allocation, and lack general OS functionalities to run commercial off-the-shelf applications. Below, we summarize the major challenges in retrofitting a microkernel as a general OS kernel for such emerging scenarios.  \n\nCompatibility: POSIX subset-compliant is not enough. Rebuilding the entire software ecosystem is impractical. Therefore, SOTA microkernels, such as seL4 [67] and Zircon [46], achieve minimal POSIX subset compliance by providing custom libraries, e.g., musl-libc [47], that generate inter-process calls (IPC) to OS services. However, they face deployment issues [6, 116], e.g., not being binary compatible, and implementation challenges, e.g., fork and poll, in emerging scenarios. Moreover, they can hardly reuse device drivers with affordable engineering effort and uncompromised performance, which are crucial for production deployment.  \n\nPerformance: IPC is not the only concern. Performance is the top priority in emerging scenarios, directly determining user experiences. While SOTA microkernels like seL4 [67] and recent architectural support [28, 49, 86] have achieved record-high IPC performance, we observe that they still cause non-trivial performance overhead because IPC frequency is significantly increased when microkernels go general $70\\mathrm{x}$ higher in smartphones than routers). Further, we observe equally severe performance issues caused by state double bookkeeping due to the multi-server design, which introduces additional performance overhead ( $\\boldsymbol{2\\mathrm{x}}$ slower than Linux) and memory footprint $(35\\%)$ . Moreover, capability-based access control, which hides frequently updated kernel objects behind capabilities, can cause significant overhead due to frequent invocations. For example, it causes page fault handling to be $3.4\\mathrm{x}$ slower than Linux.  \n\nWe started the HongMeng kernel (HM) project over 7 years ago to re-examine and retrofit the microkernel into a general OS kernel for emerging scenarios. To be practical for production deployment, HM achieves full Linux API/ABI compatibility and is capable of reusing the Linux applications and driver ecosystems such that it can run complex frameworks like AOSP [42] and OpenHarmony [35] with rich peripherals. Despite the compatibility goal that may constrain its performance, HM still puts performance as its primary emphasis. Therefore, HM respects the design principles of microkernels but not to the extreme with careful compromises. Specifically, HM makes the following key design decisions.  \n\nMinimal microkernel with least-privileged and wellisolated OS services. HM retains the minimality principle by keeping only the necessary functionality in the core kernel, including thread scheduler, serial/timer drivers, and access control, and leaving all other components as isolated OS services (multi-server) outside the core kernel. In addition, HM adopts fine-grained access control to preserve the principle of least privilege for better security. As a result, HM inherits the security and reliability benefits of microkernels.  \n\nMaximizing compatibility by achieving Linux API/ABIcompliant and performant driver reuse. HM integrates existing software ecosystems by achieving full Linux API/ABI compatibility through ABI-compliant shim that identifies and redirects Linux syscalls to IPCs. Moreover, HM reuses unmodified Linux drivers via a driver container that provides Linux runtime atop HM with minor engineering effort, and eliminates critical path performance degradation by separating the control plane and the data plane with twin drivers.  \n\nPerformance first by structural supports. HM prioritizes performance without violating the architectural principles of microkernels. Specifically, HM achieves flexible composition for hierarchically relaxing the isolation between trusted services to minimize IPC overhead, and coalesces tightly coupled services to minimize IPC frequency and eliminate state double bookkeeping in performance-demanding scenarios, while maintaining the ability to separate them in security-critical scenarios. HM also supplements capabilities with performant address token-based access control, facilitating efficient cooperation like policy-free kernel paging.  \n\nWe have deployed HM on tens of millions of devices, including smart routers, smart vehicles, and smartphones, which provides not only better security and reliability but also better performance than their Linux counterparts. The critical components of $H M$ are semi-formally verified [55] by formally specifying the design and using automated verification and verification-guided testing to validate the crucial security properties, such as free of integer and buffer overflow. HM has been certified with ASIL-D [61] (for safety) and CC EAL $^{6+}$ [62] (for security). In routers, $H M$ allows $30\\%$ more client connections by reducing $30\\%$ system memory footprint. In vehicles, $H M$ achieves a $60\\%$ faster boot time and a $60\\%$ lower cross-domain latency. In smartphones, HM achieves $17\\%$ shorter app startup time and $10\\%$ less frame drops.  \n\n2 The Case for a General Microkernel  \n\n2.1 Microkernel Review  \n\nA major hallmark of microkernels is the minimality principle [73,76], which minimizes functionality in the core kernel and moves other functions to userspace services. SOTA microkernels also adopt capability-based fine-grained access control [46,52,67,74] to preserve the least privilege principle. As a result, microkernels are inherently more secure, reliable, and extensible than monolithic kernels [12,79].  \n\nHowever, although microkernels have been extensively studied for decades [16, 30, 52–54, 64, 67, 73, 75, 76, 122], SOTA microkernels primarily target specific domains, such as embedded and safety-critical systems. Examples include L4- embedded in Qualcomm cellular modem chips [30], QNX1 in cars and embedded systems [54], and Zircon (kernel of Fuchsia) in smart speakers [46]. There has been little study on how microkernels could be extended as general OS kernels for emerging scenarios like smart vehicles and smartphones.  \n\nThe industry adopted hybrid kernels such as Windows NT [88] and Apple XNU [4], which combine a core microkernel, e.g., Mach in XNU, with all other services (as a whole) in the kernel space, e.g., Executive in NT and BSD in XNU. Although hybrid kernels also minimize functionality in the core kernel, they do not inherit many advantages of microkernels. For example, OS services in hybrid kernels are not least privileged and not well isolated. Thus, any compromised or buggy OS services can corrupt the system [88], potentially causing severe consequences, such as corrupting user data.  \n\n2.2 Demand for a General Microkernel  \n\nEmerging scenarios like smart vehicles and smartphones demand rich peripherals and applications. For example, the industry standard of vehicles has evolved to require richer OS functionalities [7]. Meanwhile, emerging scenarios also emphasize security and safety. For instance, vehicles require high reliability for passenger safety, and smartphones require enhanced security to protect sensitive data. We list the major differences from domain-specific scenarios below.  \n\nSoftware ecosystem. In domain-specific scenarios, applications are mostly customized and source-available. Thus, being POSIX-compliant is believed to be sufficient for application transplanting (not even true based on our deployment experiences). However, in emerging scenarios like smartphones, apps and libraries are typically distributed in binary form, and frameworks require more than POSIX compliance [6], which mandates Linux ABI compatibility.  \n\nResource management. In domain-specific scenarios, there are only a few pre-determined applications, and the hardware resources are limited. Therefore, applications mostly manage resources themselves, and the kernel is primarily responsible for reserving resources. In emerging scenarios, however, competing applications require coordinated resource management. The kernel requires more fledged functionalities such as efficient resource management and fair allocation.  \n\nPerformance. In domain-specific scenarios, microkernels prioritize security and strict resource (e.g., timing) isolation for mostly static applications, where performance is not a primary concern. In emerging scenarios, however, performance is also a top priority, which directly determines the user experience and, thereby, the widespread deployment of the kernel.  \n\nThe call for integrating both rich software ecosystems and functionalities, as well as security and reliability, makes it difficult for existing OS to satisfy them simultaneously. One approach would be customizing a stock OS such as Linux for such scenarios, which is unfortunately very expensive to evolve with upstream (section 2.3). Previous work also proposes various architectures, including unikernel [65,81,102], multikernel [9], exokernel [31], and splitkernel [109]. However, they primarily target server scenarios with clear resource separation while lacking support for efficient and coordinated resource management required in emerging scenarios. Moreover, the synchronization overhead and complexity introduced by split states make it challenging to achieve compatibility.  \n\nTherefore, we believe it is worthwhile to explore another avenue of evolving the microkernel into a general OS kernel.  \n\n2.3 Issues with Linux  \n\nLinux has dominated the server and cloud markets and is increasingly penetrating other domains such as PC and embedded. However, it comes at the cost of compromised security, reliability, and performance, especially in emerging scenarios.  \n\nSecurity and Reliability. Linux modules such as file system (FS) and device drivers cover about $80\\%$ of its 30 million line code base. They contribute to the majority of defects and vulnerabilities $90\\%$ of the total $1000\\,\\mathrm{{CVE}}$ [23] in the last 4 years) and significantly reduce reliability and security [19].  \n\nAdditionally, about $80\\%$ of these CVEs are data leaks that can be avoided with proper isolation. Therefore, a long line of research [18,25,38,48,56,83,90–92,100,105,106,112,120,123] aims at isolating the kernel from the modules in a compartmentalized manner. However, the inherent tight coupling requires significant engineering effort and even rewriting [56, 90, 91]. Moreover, the instability of kernel module APIs and security patches force frequent upgrades, making them less practical for real-world deployments.  \n\nGenerality vs. Specialization. While Linux targets general scenarios, recent patches and features witness that innovations are primarily driven by servers and the cloud, which even hamper the performance of other scenarios [89,103]. Moreover, the growing diversity of devices with rich peripherals and varied scenarios call for specialized strategies to exploit the performance and energy efficiency headroom, such as allocating resources according to the quality of service [20,21] or minimizing space usage [119]. However, such strategies require significant engineering effort to customize the kernel due to the inherent tight coupling of kernel modules. While there is much effort [58,66,78,84,93] to improve customizability, it is hard to integrate them into the mainstream kernel.  \n\nCustomization vs. Evolution. Another issue is evolving the custom code. Synchronizing with upstream requires significant effort to reapply the changes, while not synchronizing may expose the system to security vulnerabilities. Years of production experience suggest that it is expensive due to the frequent changes in kernel internal APIs, and performance regressions require substantial effort to locate and even redesign the entire patches. This severely limits customizability in real-world deployments. Hence, a massive amount of products on the market are still running Linux 2.6 [50,51,117], which reached End-of-Life (EOL) 7 years ago [114] and has many known security vulnerabilities [24,117].  \n\n3 Revisiting Microkernel for Going General  \n\n3.1 Microkernel at Scale  \n\nDeploying a microkernel in emerging scenarios poses challenges in both performance and compatibility. Figure 1 presents the observed characteristics of emerging scenarios from deploying HM in productions. For routers, we collected data directly from the production environment. For vehicles and phones, we replayed a typical usage (lasting 24 hours) derived from recorded massive amount of real-world executions at scale (anonymous and with user consent).  \n\nObservation 1: IPC frequency increases rapidly in emerging scenarios. Figure 1a shows the IPC frequency CDF in HM when configuring all OS services to be isolated in userspace. Smartphones (avg. 41k/s) and vehicles $(7\\mathrm{k}/\\mathrm{s})$ have a much higher IPC frequency than routers $\\mathrm{(0.6k/s}$ , more similar to domain-specific scenarios). Figure 1b, 1e, and 1f illustrate it by showing the minor (i.e., not from disk/device)  \n\n  \nFigure 1: Characteristics of emerging scenarios obtained from the deployment of HM in tens of millions of devices. All OS services in HM are configured to be well-isolated in userspace.  \n\npage faults’ frequency and the distribution and frequency of syscalls in phones. As shown in the figures, the high IPC frequency is not only caused by the higher syscall frequency $\\mathrm{^{61k/s}}$ , $13\\mathbf{x}$ higher than routers), but also by invoking massive amounts of file operations (IPC to the FS), and triggering numerous page faults on memory-mapped flies $(5\\mathrm{k}/\\mathrm{s})$ , which requires an additional IPC roundtrip between the memory manager and the FS. Hence, we should not only optimize IPC performance but also minimize the IPC frequency.  \n\nObservation 2: Distributed multi-server causes state double bookkeeping. The minimality principle determines that there is no centralized repository for shared objects, such as the flie descriptor (fd) and page caches, and distributes them in multiple places. However, as shown in Figure 1c-1e, applications in emerging scenarios frequently invoke functions like poll that rely on the centralized management of such states. Figure 2 further presents the CPU flame graph of application startup, which relies heavily on the performance of flie mapping and is crucial to the user experience [45]. As marked in the figure, $16\\%$ of the time is spent on handling page cache misses, which introduces an additional IPC roundtrip and is $2\\mathbf{x}$ slower than Linux. Moreover, the double bookkeeping of page caches consumes an additional 50MB of memory on top of the 120MB base $\\mathrm{(FS+mem)}$ in smartphones.  \n\nObservation 3: Capabilities inhibit efficient cooperation. Capabilities, which hide the kernel objects behind them, introduce significant performance overhead due to the frequent updating of some kernel objects (e.g., the page table)  \n\n  \nFigure 2: CPU flame graph of smartphone app startup in HM. Services coalescing and kernel paging are disabled.  \n\nmanaged outside the kernel and inhibit efficient cooperation between them. For example, this may cause the handling of anonymous page faults $3.4\\mathrm{x}$ slower than Linux, which frequently occurs in smartphones (avg. $27\\mathrm{k}/\\mathrm{s}$ , $80\\%$ of minor page faults in Figure 1b) and adds a non-trivial overhead to the app startup time ( ${}^{4\\%}$ in Figure 2).  \n\nObservation 4: Eco-compatibility requires more than POSIX compliance. Many SOTA microkernels achieve a minimal subset of POSIX compliance by providing custom runtime libraries [47] that link directly to applications and generate IPC to OS services. However, it faces deployment issues of being not binary compatible and requiring a customized building environment. Moreover, since Linux uses file as a unified interface, which no longer exists in the microkernel, it is also challenging to implement efficient fd multiplexing like poll and vectored syscalls like ioctl, which are frequently used in emerging scenarios as shown in Figure 1c-1e.  \n\nObservation 5: Deployment in emerging scenarios requires efficient driver reuse. When deploying HM on smartphones, we observe a massive increase in the number of drivers required to function correctly. For routers, fewer than 20 drivers are required (primarily maintained in-house), which increases to more than 700 for vehicles and phones. Our estimates indicate that it would take more than 5,000 person-years to rewrite those drivers, and it takes time to get mature and keep evolving. Thus, reusing device drivers is a more reasonable option. However, previous work, including transplanting the runtime environment of drivers [3, 17, 32, 41, 118] and using virtual machines [72], faces compatibility, engineering effort, and performance challenges (discussed in section 5.2).  \n\n3.2 Overview of HongMeng  \n\nHM respects the core design principles of microkernels but not to the extreme, with careful compromises to address the performance and compatibility challenges in emerging scenarios. We summarize HM’s design decisions in Table 1 and list design principles below. Figure 3 shows HM’s overview.  \n\nPrinciple 1: Retain minimality. The security, reliability, and extensibility of microkernels derive from three fundamental architectural design principles, including separating policy and mechanism, decoupling and isolating OS services, and enforcing fine-grained access control. Hybrid kernels also enforce minimality through code decoupling but without proper isolation. Thus, it fails to inherit the major benefits of mi\n\nMinimalitySOTA Microkernels Minimal Kernel IPC w/FastpathHybrid Kernels Code DecouplingHongMeng's Design Retained: Minimal microkernel with isolated, least-privileged OS services. Enhanced:SynchronousRPCaddressesresource alloc./exhaustion/acct.issues.IPC Isolation Composition Access Control Memory App Interfaces Device DriverUserspace Services Static Multi-server Capability-based Paging in Userspace POSIX-compliant Transplanting/VMFunction Call Coalesce w/ Kernel Static Single Server Object Manager Paging in Kernel POSIX+BSD/Win Native DriverFlexibilized: Differentiated isolation classes for tailored isolation and performance. Flexibilized:Flexible compositionto accommodatediversescenarios. Extended:Address tokens enable efficient kernel objects co-management. Enhanced: Centralized management in a service with policy-free paging in kernel. Extended:LinuxAPI/ABI compatible via anABI-compliant shim. Enhanced:Reusing Linux drivers efficiently via driver container with twin drivers.Isolation Class 0 Isolation Class 1 Core TCB Mechanism-enforcedIsolationIsolation Class 2 Address Space Isolation Application1overhead by relaxing the isolationbetween trusted OS ser- vices (section 4.2). HM also coalesces tightly coupled OS services (① in Figure 3) to minimize the IPC frequency in performance-demanding scenarios (section 4.3). Moreover. HM enables efficient kernel objects co-management (?) by grates with existing software ecosystems by achieving Linux ABI compliance through a shim(?) that redirects all Linux syscalls to appropriate OS services and serves as a centralAOSP/OpenHarmonyApp BinaryCompatible AOSP/OpenHarmonyLinux Driver Container LinuxRuntime LinuxDrivers DC-BaseNetwork StackApplication 2 Resource ManagementELO/Ring3 Linux EL1/Ringo SyscallContext SwitchCustom LibC (POSIX API)Memory Pool Custom LibC3ABl-compliantShim Coalesce Services Gate Control PlaneNativesupplementing capabilities with address tokens (section 4.4), which facilitates policy-free in-kernel paging of anonymous memory (section 4.5).File Mem Mgr.Proc.Native DriverCont.Driver Cont.Mem File Mgr. System ②SystemMgr.Twin DriverRPC-likeIPCFlexibilized: Prioritize performance by providing structural Page Table2Co-managed KernelObject PageTable (a) SmartphoneDataPlaneCoreKernelsupports for fexible assembly to adapt to diversescenarios. Principle 3: Maximizing eco-compatibility. HM inte-CoreKernelHardware Figure 3: HongMeng overview. (a) and (b) show its composi- tion in smartphones and routers. Different colors imply differentHardware (b) Smart Router  \n\nisolation classes. $\\pmb{\\mathrm{\\Sigma}}$ coalesces coupled services. Address tokens enable kernel objects co-management $\\pmb{\\varphi}$ . ABI-compliant shim $\\pmb{\\otimes}$ enables binary compatibility. Driver container $\\pmb{\\mathbb{\\otimes}}$ reuses Linux drivers efficiently via data/control plane separation $\\pmb{\\mathcal{\\Theta}}$ .  \n\ncrokernels. Therefore, while emphasizing compatibility and performance, HM respects the architectural design principles of microkernels.  \n\nHM keeps only minimal and necessary functionality in the core kernel, including thread scheduler, serial and timer drivers, and access control. All other functionality is implemented in isolated OS services, such as process/memory manager, drivers, and FS. Moreover, HM adopts fine-grained access control to preserve the least privilege principle. Services can only access strictly restricted resources (kernel objects) necessary for functionality. As such, HM inherits the security, reliability, and extensibility of microkernels.  \n\nRetained: Minimal microkernel with well-isolated and leastprivileged OS services.  \n\nPrinciple 2: Prioritize performance. The promising beneftis of microkernels are compromised by architecture-inherent performance issues in emerging scenarios. Therefore, instead of enforcing uniform but overly strong isolation, HM provides structural support for assembling the system to satisfy both the performance and the security requirements. In particular, besides adopting an RPC-like fastpath that addresses the resource allocation/exhaustion/accounting issues (section 4.1), HM proposes differentiated isolation classes to reduce IPC to efficiently support functions like poll (section 5.1). Moreover, HM reuses unmodified Linux device drivers via driver container $(\\pmb{\\Theta})$ , which provides the necessary runtime derived directly from the mainline Linux with minor engineering effort (section 5.2). HM further improves drivers’ performance by exploiting control and data plane separation $(\\pmb\\otimes)$ .  \n\nEnhanced: Maximize compatibility by achieving Linux API/ABI-compliant and performant driver reuse.  \n\nHM’s Threat Model. HM retains the architectural design principles of microkernels, thus maintaining a similar threat model to SOTA microkernels, which prevents malicious applications and OS services from accessing other’s memory and ensures the confidentiality, integrity, and availability (CIA) properties of data, with the following differences.  \n\nFirst, since applications in emerging scenarios require centralized memory management for compatibility reasons (section 4.5), the memory manager (the only exception), including its coalesced services (only FS in phones on deployment), can inevitably access applications’ address spaces. Besides, in safety-critical scenarios where memory is self-managed, HM does not create such a centralized memory manager.  \n\nMoreover, for the sake of performance, there are compromises on additional attack surfaces (section 4.2), different partitioning of failure domains (section 4.3), and additional data leakage possibilities on carefully selected objects (will not corrupt the kernel, section 4.4). The detailed security design will be discussed in the corresponding section.  \n\n4 Performance Design of HongMeng  \n\n4.1 Synchronous RPC-like IPC Fastpath  \n\nMicrokernels use IPC to invoke OS services. A long line of research has proposed numerous optimizations to minimize IPC overhead. However, when applying them to emerging scenarios, we encountered several severe issues, either previously neglected or caused by changed assumptions. HM carefully addresses these issues, as summarized in Table 2.  \n\nTable 2: Comparison of IPC in HM.   \n\nIPCFastpathMigrationHongMeng IPCBypass SchedulingYesYesYesReducedSwitchesN/ARegistersReg./AddressSpace/Priv.ResourceAllocationPre-allocPre-allocPre-bind&AdaptiveResourceExhaustionBlockedBlockedReservedforReclaimingResourceAccountingTemporalTemporalTemp./Energy/Memory  \n\nSynchronous RPC or Asynchronous IPC. IPC typically assumes symmetric endpoints with the same execution model. Therefore, previous work suggests that asynchronous IPC can avoid serialization on multicore [30], allowing both endpoints to continue execution without blocking. However, in emerging scenarios, we observe that most IPCs are procedure calls, where the caller and callee can be clearly identified. Furthermore, OS services are mostly invoked passively rather than working continuously, and most subsequent operations of the application depend on the results of the procedure call. Therefore, synchronous Remote Procedure Call (RPC) is a more appropriate abstraction for service invocations.  \n\nHM adopts an RPC-like thread migration [33, 94] as the IPC fastpath for service invocations. When sending an IPC, the core kernel performs a direct switch (bypassing scheduling, similar to prior work [10, 30, 49, 67, 70]) and switches only the stack/instruction pointer (avoids switching other registers) as well as the protection domain. Specifically, HM requires OS services to register a handler function as the entry point and to prepare an execution stack pool. When an application invokes a service, the core kernel records the caller’s stack/instruction pointer in an invocation stack and switches to the handler function with the prepared execution stack. On return, HM pops an entry from the invocation stack and switches to the caller. The IPC arguments are primarily passed through registers, with additional arguments passed through shared memory.  \n\nPerformance gap. Although HM bypasses scheduling and avoids switching registers, it still faces non-trivial performance degradation due to privilege level/address space switching and cache/TLB pollution [9,30,49,86] (accounts for $50\\%$ of total IPC cost). We further bridge this performance gap using differentiated isolation classes in section 4.2.  \n\nResource Allocation. The memory footprint of IPC has been largely neglected by previous work. However, due to the extremely high IPC frequency and massive number of connections ${}>\\!1\\mathrm{k}$ threads simultaneously) in emerging scenarios like smartphones, we find it essential to consider IPC’s memory footprint in production, as it can cause severe problems such as out-of-memory (OOM) and even system hangs. Although each IPC connection in HM requires only an individual execution stack (rather than a full-fledged thread with all related data structures), its memory footprint is still non-trivial, given the massive amount of IPCs.  \n\nPrevious work pre-allocates a thread/stack pool of a fixed size and binds it to connections. However, its size is hard to decide due to the diversity and dynamism of workloads, including the number of running threads and requirements for different OS services. A large pool would quickly drain the memory, while dynamic allocation on connection introduces runtime overhead on the critical path of IPC. We initially tried to prepare and bind stacks in each OS service for each thread on creation. However, we quickly realized that the problem still exists because some services are barely used by some threads (wasted), and there exist many IPC chains (to another OS service) that need another stack.  \n\nTherefore, HM strikes a sweet spot by pre-binding stacks in frequently-used OS services (e.g., process/memory manager and FS) for each thread while still maintaining a stack pool whose size is adjusted adaptively at runtime. When the remaining stacks fall below a threshold, the OS service will allocate more to reduce synchronous allocation. HM further reduces its memory footprint by reusing the same stack when calling the same service (e.g., ABA-like call) in an IPC chain.  \n\nResource Exhaustion. IPC can fail due to resource exhaustion. Specifically, when the stack pool runs out while OOM occurs, OS services cannot allocate a new stack to process the IPC request. However, apps cannot handle such an error (not existing in a monolithic kernel). Therefore, such requests are queued (blocked) in SOTA microkernels, which may cause severe issues like circular wait and even system hangs.  \n\nAn intuitive approach is to send another IPC to the memory manager to reclaim some memory synchronously. However, we find that under such a scenario (already OOM), the IPC to the memory manager is likely to fail again. Such a failure is likely to occur in emerging scenarios where workloads are non-deterministic and heavy loads occur frequently (e.g., opening multiple apps simultaneously).  \n\nHM mitigates this by reserving an individual stack pool. Once OOM occurs, the kernel will synchronously IPC to the memory manager using the pool for memory reclaim (repeatedly) until the user’s IPC succeeds. Thus, applications IPCs are guaranteed to be handled correctly.  \n\nResource Accounting. IPC assumes a different execution entity when handling requests, thus attributing the consumed resource to OS services. However, since competing applications in emerging scenarios require a clear accounting of resources, the consumed resource should be precisely accounted to the caller app. Previous work achieves temporal isolation by inheriting the caller’s scheduling context [70, 80]. However, emerging scenarios also require an accounting of both energy and memory consumption. Therefore, HM records the identity of the user app (root caller in the IPC chain), and attributes the consumed resources to it when handling IPC.  \n\n  \nFigure 4: Round-trip IPC latency between ICx & ICy (ICxICy) in Raspberry Pi 4b. IC0 includes the core kernel. IC2 includes user apps. Zircon cannot run on Pi4b and is several times slower [49].  \n\nDecision: Supplement async./sync. IPC with an RPC-like fastpath for invoking OS services while carefully addressing the resource allocation/exhaustion/accounting issues.  \n\n4.2 Differentiated Isolation Classes  \n\nIsolation of OS Services. Placing all OS services in userspace may improve security, but it fails to meet performance requirements in emerging scenarios. We observe that not all services require the same class of isolation. In particular, mature, verified, and performance-critical OS services can be subjected to weaker isolation for optimal performance in practical deployments. Moreover, rapidly evolving services may frequently introduce bugs and vulnerabilities, thus requiring more robust isolation to prevent kernel corruption. OS services with large codebases and cumbersome features, such as drivers, require isolation to reduce the trusted computing base (TCB).  \n\nTherefore, HM adopts differentiated isolation classes (IC) to provide tailored isolation and performance for different OS services. Specifically, isolation classes classify services and define the isolation between them. Figure 4 shows the round-trip IPC latency between services at different isolation classes, compared to seL4 [67] and Fiasco.OC [69].  \n\nIsolation Class 0: Core TCB. IC0 applies to carefully verified, extremely performance-critical, trusted OS services, such as the ABI-compliant shim (the only IC0 service in deployment). No isolation is enforced between these services and the kernel. Therefore, IPCs are all indirect function calls.  \n\nIC0 Threat Model: IC0 is part of the core TCB, and any compromised IC0 services can arbitrarily read and modify others’ memory. Therefore, placing services at IC0 should be carefully validated to avoid core kernel corruption.  \n\nIsolation Class 1: Mechanism-enforced Isolation. IC1 applies to performance-critical and validated OS services. Inspired by previous intra-kernel isolation approaches [11,49, 59,71,112,120], HM places these services in the kernel space and uses mechanisms to enforce isolation between services. Specifically, HM carefully divides the kernel address space into distinct domains and assigns each service a unique domain (IC0/core kernel also resides in a unique domain). HM uses ARM watchpoint [63] and Intel PKS [60] to prevent cross-domain memory access. Moreover, since IC1 services run in kernel space, they can execute privileged instructions. To prevent this, HM adopts binary-scanning and lightweight control-flow integrity (CFI, leveraging ARM pointer authentication (PA) [77]) to ensure services cannot execute illegal control flows that contain privileged instructions, and uses a secure monitor [49,108] to guard the page table against code injection, which also traps any privileged instruction through VM Exits as a complement to CFI.  \n\nIPC between IC1 services (or to IC0) will enter a gate in the core kernel that performs a minimal context switch (switch instruction and stack pointers, w/o address space switching and scheduling) and configures the hardware to switch domains (take only a few cycles). Such a gate cannot be bypassed since domain switches require privileged instructions. Therefore, the IPC overhead is significantly reduced. As shown in Figure 4, it reduces the IPC latency between IC1 services by $50\\%$ compared to userspace services (IC2IC2).  \n\nIC1 Threat Model: IC1’s threat model differs from other multi-server microkernels by assuming the correctness, soundness, and security of the applied isolation mechanism, which does expose some additional attack surfaces. For example, there are new attacks on ARM PA emerged recently [22]. Besides that, IC1 shares the same threat model, which prohibits any compromised service from reading/writing the core kernel’s memory (and other OS services’) and executing privileged instructions.  \n\nIsolation Class 2: Address Space Isolation. IC2 applies to non-performance-critical services or those containing thirdparty code (e.g., Linux drivers), enforced by address space and privilege isolation. IPC between IC2 services in HM (IC2IC2) is slightly slower than in seL4, mainly due to fine-grained locking, which is essential for scaling to multi-core processes under real-world loads.  \n\nIC2 Threat Model: IC2 shares exactly the same threat model as other multi-server microkernels.  \n\nAlthough IC1 significantly reduces the IPC overhead, it also introduces additional attack surfaces and has resource limitations (e.g., 16 domains in Intel PKS, 4 domains in ARM Watchpoint). Therefore, only performance-critical and validated OS services are placed at IC1. In addition, HM can quickly move all services back to IC2 if new attacks emerge. We further discuss deployment experiences on configuring isolation classes in section 4.3. Moreover, IC0/1 does not imply coupling to the kernel. The isolation classes allow for configurable isolation decisions during deployment rather than an isolation assumption during development. Different scenarios use different configurations, as shown in Figure 3.  \n\nDecision: Not all OS services require the same class of isolation. Adopt differentiated isolation classes to relax isolation between trusted services for improved performance.  \n\n4.3 Flexible Composition  \n\nPartitioning of OS Services. Although intuitively, OS services should be well-decoupled, e.g., FS and memory manager, we observe that OS services are asymmetric since some functionalities require close cooperation between specific services. For example, the FS is not the only entrance to accessing a file. POSIX supports file mapping that reads files through the memory manager, and it frequently appears on the critical path and significantly affects the user experience.  \n\nThe isolation classes enforce the same isolation between same-class OS services. Therefore, without further structural support, $H M$ still faces performance degradation compared to the monolithic kernel. First, the frequently invoked IPCs between tightly coupled services still cause noticeable overhead $20\\%$ in page fault handling for memory-mapped flies) even in IC1 (kernel space). Moreover, double bookkeeping of shared states, such as page caches, introduces significant memory footprint and synchronization overhead. Finally, there is no global view of page caches to guide resource recycling (e.g., Least Recently Used, LRU).  \n\nTo bridge the performance gap, HM adopts a configurable approach that allows coalescing tightly coupled OS services in performance-demanding scenarios, trading off isolation for better performance, while retaining the ability to separate them in safety-critical scenarios. When coalesced, no isolation is enforced, and IPCs between two services become function calls, while others remain as they are (well-isolated).  \n\nCoalescing also enables efficient co-management of page caches. Instead of maintaining them in both the FS and the memory manager, they can be co-managed when coalesced. It eliminates double bookkeeping and synchronization overhead and provides a global view for efficient recycling. To retain the ability to separate them, we provide a mechanism to automatically convert accesses of shared page caches into IPCs when separated. However, it will introduce non-trivial overhead. Therefore, in deployment, we implement both versions (sep./shr.) manually and enable them accordingly.  \n\nPerformance. As shown in Table 3, when coalescing the FS with the memory manager, replacing the IPC reduces the latency of handling page faults caused by page cache misses by $20\\%$ (Sep. Cache). It can be further reduced by $30\\%$ (Shr. Cache) and achieves similar performance with Linux (5.10, detailed in section 6.2) by co-managing the shared page caches. Coalescing also speeds up the write throughput of tmpfs by $40\\%$ . Moreover, the memory footprint of coalesced services is reduced by $37\\%$ ( $\\mathrm{FS+}$ memory) in smartphones.  \n\nSecurity. The coalesced services are in a single failure domain, whose threat model (as a whole) remains the same as the isolation class in which it resides. Therefore, any failed or compromised service can only corrupt its coalesced services, which is also the primary compromise for performance. Hence, service coalescing should be carefully evaluated. In practice, due to the extremely high frequency of file operations in smartphones (Figure 1e), their performance targets can only be achieved by coalescing the FS with the memory manager. However, the security is still improved (isolated from other services) compared with monolithic kernels.  \n\nDeployment Experiences. Together with the differentiated isolation classes, HM enables flexible composition, allowing the key components to be assembled flexibly (user-space or kernel-space, separated or coalesced), enabling exploration of tradeoffs between isolation and performance according to scenarios’ requirements, and the ability to scale from routers to smartphones with the same code base. The evolution of HM witnesses such explorations. Initially, all services were isolated at IC2. To meet the performance goal, we carefully assemble the system to retain most security properties by preserving the following rules.  \n\nTable 3: Performance improved by coalescing the FS service and the memory manager in the big core of Kirin9000 [57].   \n\nSeparatedCoalescedLinuxPage Fault (Cycles)70925290 (Sep.Cache) 3785(Shr.Cache)3432Tmpfs Write (MB/s)149220672133MemoryFootprint (MB)190120N/A\n\nTable 4: Address tokens support most operations of capabilities and allow direct access, except restricting fine-grained operation and chain revocation.  \n\nCapabilitiesAddressTokensTokenCSlotidMappedAddressAccessDelegatetoKernelDirect(RW)/writev(RO)OwnershipCaps in CNodeMappedPagesGrantMovetoCNodeMapPagetoVSpaceRevokeRemovefromCNodeUnmapPageChainRevokeSupportNo supportSecurityMonitoralloperationsRestrictions on mapped Obj.  \n\nFirst, due to the additional attack surfaces, IC1 services cannot contain any third-party code. Thus, although some drivers are also performance-critical, we kept them at IC2 and sped up via control/data plane separation (section 5.2). Second, service coalescing, especially with the memory manager, undeniably weakens isolation and security (though still improved compared with monolithic kernels). Therefore, we leave it configurable and only enable it on phones. Moreover, IC0 not only increases the core TCB but also has strict memory limitations and non-blocking requirements. Thus, in practice, HM only places the ABI shim (which can be opted out) in IC0. Section 6.1 details the configurations.  \n\nDecision: OS services are asymmetric. Coalesce tightly coupled OS services and flexibly assemble the system to meet diverse requirements in various scenarios.  \n\n4.4 Address Token-based Access Control  \n\nSOTA microkernels make all kernel objects explicit and subject to capability-based access control [30] to preserve the principle of least privileged, which is primarily implemented in a partitioned fashion that keeps a token (typically a slot ID) in userspace representing the permission to access a kernel object. However, we encountered severe performance issues when deploying it in emerging scenarios.  \n\nClear relationship but slow access. Although capabilities are effective in describing the external relationships of kernel objects, i.e., the authorization chain, accessing their internal contents requires sending the token with the operations to the core kernel, which introduces non-trivial performance overhead due to privilege switches and accesses to multiple metadata tables. Kernel objects are hidden behind the capabilities and are only accessible by the core kernel. However, due to the minimality principle, the content of some kernel objects (e.g., page tables) should be frequently updated by OS services outside the core kernel, for which partitioned capabilities are no longer efficient.  \n\nSome microkernels speed up access by mapping specific objects to userspace. However, they can only be applied to few limited objects (e.g., memory objects [30,67], part of the thread control block [3,9,76], and kernel interface page [76]) for security and lack the ability to synchronize data correctly, which inhibits the cooperation between the kernel and OS services. To address these issues, HM proposes a generalized address token-based approach that can be applied to a broader range of objects, enabling efficient co-management.  \n\nSpecifically, as shown in Figure 5, each kernel object is placed on a unique physical page in HM. Granting a kernel object to an OS service requires mapping such a page to its address space $(\\pmb{\\mathbb{0}})$ . Thus, the mapped address serves as the token to access the kernel object directly from the hardware without involving the kernel (unwillingly). Kernel objects can be granted (mapped) as read-only (RO) or read-write (RW). OS services can read RO kernel objects without kernel involvement. To update them, a new syscall, writev, should be used, passing the target address with the updated value, and the core kernel will verify permissions by referring to the kernel object’s metadata $(\\pmb{\\varphi})$ . For RW kernel objects, once granted, can be updated by OS services without kernel involvement $(\\pmb{\\otimes})$ . Moreover, for objects smaller than a page with the same property (permission) and a similar life cycle, HM batches these objects into a single page upon allocation, allowing them to be granted and revoked collectively.  \n\nFunctionality. Address tokens support most operations of capabilities, as shown in Table 4 (compared to seL4 [107], Zircon has similar functionality [37]), with two exceptions. First, address tokens cannot restrict fine-grained operations once granted, which weakens security and exposes additional attack surfaces. Besides, capabilities store the detailed relationship, allowing chain revocation, which address tokens do not support due to implicit ownership. Nevertheless, address tokens are only used by selected co-managed kernel objects. The attack surfaces are carefully mitigated (discuss below). Moreover, due to the centralized resource management, kernel objects have specific owners (will not be granted to others). Thus, chain revocation is rarely used.  \n\nSecurity. Once an address token is granted to an OS service, the kernel cannot monitor the subsequent operations. HM mitigates this by restricting the objects mapped to userspace (enforced by static analysis). Only kernel objects that exclusively contain the values of certain variables in kernel-preserved structures (pointers are not allowed to prevent the time-to  \n\nOS ServicesTokens DirectUpdate KOAddrNew Value Co-managedKOAddr2 Co-managed KernelWritevsyscall 3 MaptoGrant VerifyObject New ValueKernel ObjectCore Kernel(RW/RO） KernelObjectManagerRO MetadataRW Metadata  \n\nFigure 5: Address token-based access control in HM. ❶Map kernel object’s page to grant. $\\pmb{\\varphi}$ Direct access to RW objects. $\\bar{\\bf\\otimes}$ Use writev to update RO objects, verified by the kernel.  \n\n  \nFigure 6: Latency of accessing kernel objects on Raspberry Pi 4b. Addr-rd/wr represent address tokens in HM. ROAddr-wr represents writing to read-only objects in HM.  \n\ncheck to time-to-use attack) are mapped RW (e.g., PCache in section 4.5), ensuring they will not corrupt the kernel with incorrect or inconsistent data. The rest of the kernel’s internal states (e.g., pointers and reference counters) can only be mapped RO or not granted at all to prevent it from being corrupted. HM further applies a sanity check when reading from RW objects. It does introduce some attack surfaces by leaking kernel-internal information, which can be mitigated by hardware encryption like ARM PA.  \n\nSynchronization. There are two approaches to sharing data between OS services and the kernel leveraging address tokens. First, OS services and the kernel can exchange messages asynchronously (message-passing). For example, PCache in section 4.5 sends pre-allocated pages to the kernel for future kernel paging. HM uses a lock-free ring buffer to synchronize the data correctly. Besides, OS services can apply in-place updates to the objects (e.g., VSpace in section 4.5, which stores the memory layout) that the kernel may read concurrently. HM adopts fine-grained locking to ensure correctness. However, it may block the kernel when the service is preempted while executing critical sections. Therefore, the kernel can only use the trylock operation on RW-mapped objects. If it fails, HM will redirect to the OS services (slow path) to finish the procedure (e.g., paging in section 4.5).  \n\nPerformance. Figure 6 compares the latency of accessing kernel objects after applying address tokens. The reading and writing (to RW) latencies are significantly reduced compared with capability-based approaches. However, the latency of writing RO objects is slower than seL4 on RPi4b, mainly due to the use of AT instruction on ARM to translate the address and check the permissions, which is slow on RPi4b (yet optimized in the advanced smartphone chips).  \n\nUsage Scenario. For security concerns (see above), address tokens are OS-internal abstractions that supplement the capabilities for efficient co-management with OS services. Specifically, besides enabling direct updates to kernel objects managed by services, it allows them to read internal states (e.g., poll list in section 5.1) without kernel involvement, sim", "files_in_pdf": [{"path": ".pdf_temp/viewrange_chunk_1_1_5_1762082320/images/xpd4vf.jpg", "size": 176689}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762082320/images/bvyp9s.jpg", "size": 78688}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/ozjcns.jpg", "size": 46767}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/du0isb.jpg", "size": 48396}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/qmrunj.jpg", "size": 573286}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/6g298c.jpg", "size": 52578}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/qk4lta.jpg", "size": 20940}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/5iqnf7.jpg", "size": 27067}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762082322/images/4t60pl.jpg", "size": 26965}]}